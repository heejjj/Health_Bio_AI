{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6WNgbv66fsx+0v2PFcR/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heejjj/Health_Bio_AI/blob/bio_colab/Hyperparameter_optimization_0421.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## iris data로 xgboost 적용"
      ],
      "metadata": {
        "id": "PnLP23_GedML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dYAN8Twud1r_"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "#Loading iris dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "#independent feautres\n",
        "X = iris.data\n",
        "\n",
        "# target features\n",
        "y = iris.target\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#xgboost의 단점: 복잡한 파라미터가 있음음\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "clf = XGBClassifier()"
      ],
      "metadata": {
        "id": "ry9lnPNrfEB8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid SearchCV\n",
        " - 사용자가 하이퍼파라미터마다 몇가지 값을 가진 리스트를 입력하면,가능한 하이퍼파라미터의 경우의 수마다 예측 성능을 측정하여 사용자가 일일이 하이퍼 파라미터를 설정하고, 예측성능을 비교하여 최적의 파라미터를 찾는 수고를 줄이고 이 과정을 한꺼번에 진행한다\n",
        "\n",
        "  -장점: 구간을 줄 수 있다. "
      ],
      "metadata": {
        "id": "J9wjvQ8BfLT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn import model_selection\n",
        "\n",
        "#define a set of values as a dictionary for hyperparameters\n",
        "\n",
        "param_gird = {\n",
        "    'n_estimators': [100,200,300,400],\n",
        "    'max_depth': [1,3,5,7],\n",
        "    'reg_lambda':[0.1, .1, .5]\n",
        "}\n",
        "\n",
        "#declaring GridSearchCV model\n",
        "\n",
        "model = model_selection.GridSearchCV(\n",
        "    estimator = clf,\n",
        "    param_grid = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    verbose = 10,\n",
        "    n_jon\n",
        ")"
      ],
      "metadata": {
        "id": "6X6cjspRfion"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#베이지안optimization: 대체모델을 만듬\n",
        "#폭을 그림\n",
        "#대체 함수를 만들었을때, 가장 좋은것에 별-> acquisition function -> 추천의 후보를 줌줌\n"
      ],
      "metadata": {
        "id": "YkFRZipOhOzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayesian Optimization \n",
        " - [참고] https://wooono.tistory.com/102\n",
        "\n",
        " Bayesian Optimization은 어느 입력값(x)를 받는 미지의 목적 함수 ((f(x))를 판단하고 결정하여, 해당 함숫값(f(X))을 최대로 만드는 최적해를 찾는 것을 목적\n",
        "\n",
        " 탐색 대상함수(목적함수)와 hyperparameter pair(쌍)을 대상으로 Surrogate Model(대체모델)을 만들어, 순차적으로 hyperparameter를 update하며 평가를 통해 최적의 hyperparameter 조합을 탐색함. \n",
        "\n",
        " 이 때의 목적함수를 Black-box Function이라고 함. \n",
        "\n",
        " Bayesian Optimization에는 두 가지 필 수 요소가 있음\n",
        " 1. Surrogate Model은, 현재까지 조사된 입력값-함숫결과값 점들 $(x_1, f(x_1)),...,(x_t, f(x_t))$ 을 바탕으로, 미지의 목적 함수의 형태에 대한 확률적인 추정을 수행하는 모델을 지칭함.\n",
        " 2. Acquisition Fuction은, 목적함수에 대한 현재까지의 확률적 추정 결과를 기반으로, '최적의 입력값을 찾는 데에 있어 가장 유용할 만한' 다음 입력값 후보를 추천해 주는 함수임. "
      ],
      "metadata": {
        "id": "QGGotdJkiDoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb33tsP%2FbtraMpvxJG0%2FSn7uQK7k910IQ7cP3ZM9vk%2Fimg.png'>"
      ],
      "metadata": {
        "id": "lf-CJy92nHUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 대략적인 수행 과정\n",
        "  - 파란색 선: 목적함수 f(x)\n",
        "  - 검정색 선: 지금까지 관측한 데이터를 바탕으로, 예측한 estimated function 의미\n",
        "  - 검정색 점선 주변의 파란 영역은(Uncertain area)는 목적함수 f(x)가 존재할만한 confidence bounde(fuction의 variance)를 의미\n",
        "\n",
        "  - EI(x)는, Acquisition Function을 의미, 다음 입력값 후보 추천 시 사용됨\n",
        "  - Acquisition Fuction 값이 컸던 지점을 확인-> 해당 지점의 hyperparameter를 다음 입력값으로 사용\n",
        "  - Hyperparameter에 따라 estimated fuction을 계속 update하면, estimation fuction과 목적함수f(x)가 흡사해짐\n",
        "  - 관측한 지점 중 best point를 argmax f(x)로 선택\n",
        "\n",
        "- 자세한 수행 과정\n",
        " - 입력값, 목적 함수 및 그 외 설정값을 정의 \n",
        "  - 입력값 x : 여러가지 hyperparameter\n",
        "      - 목적 함수 f(x) : 설정한 입력값을 적용해 학습한, 딥러닝 모델의 성능 결과 수치(e.g. 정확도)\n",
        "      - 입력값 x 의 탐색 대상 구간 : (a,b)\n",
        "      - 입력값-함숫결과값 점들의 갯수 : n\n",
        "      - 조사할 입력값-함숫결과값 점들의 갯수 : N\n",
        "    - 설정한 탐색 대상 구간 (a,b) 내에서 처음 n 개의 입력값들을 랜덤하게 샘플링하여 선택합니다.\n",
        "\n",
        "    - 선택한 n 개의 입력값 x1, x2, ..., xn 을 각각 모델의 hyperparameter 로 설정하여 딥러닝 모델을 학습한 뒤, 학습이 완료된 모델의 성능 결과 수치를 계산합니다.\n",
        "\n",
        "        - 이들을 각각 함숫결과값 f(x1), f(x2), ..., f(xn) 으로 간주합니다.\n",
        "입력값-함숫결과값 점들의 모음 (x1, f(x1)), (x2, f(x2)), ..., (xn, f(xn)) 에 대하여 Surrogate Model 로 확률적 추정을 수행합니다.\n",
        "\n",
        "    - 조사된 입력값-함숫결과값 점들이 총 N 개에 도달할 때까지, 아래의 과정을 반복적으로 수행합니다.\n",
        "\n",
        "        - 기존 입력값-함숫결과값 점들의 모음 (x1, f(x1)),(x2, f(x2)), ..., (xt, f(xt)) 에 대한 Surrogate Model 의 확률적 추정 결과를 바탕으로, 입력값 구간 (a,b) 내에서의 EI 의 값을 계산하고, 그 값이 가장 큰 점을 다음 입력값 후보 x1 로 선정합니다.\n",
        "        - 다음 입력값 후보 x1 를 hyperparameter 로 설정하여 딥러닝 모델을 학습한 뒤, 학습이 완료된 모델의 성능 결과 수치를 계산하고, 이를 f(x1) 값으로 간주합니다.\n",
        "        - 새로운 점 (x2, f(x2)) 을 기존 입력값-함숫결과값 점들의 모음에 추가하고, 갱신된 점들의 모음에 대하여 Surrogate Model 로 확률적 추정을 다시 수행합니다.\n",
        "        - 총 N 개의 입력값-함숫결과값 점들에 대하여 확률적으로 추정된 목적 함수 결과물을 바탕으로, 평균 함수 μ(x) 을 최대로 만드는 최적해를 최종 선택합니다. 추후 해당값을 hyperparameter 로 사용하여 딥러닝 모델을 학습하면, 일반화 성능이 극대화된 모델을 얻을 수 있습니다.\n",
        "   "
      ],
      "metadata": {
        "id": "MhscKHWdnLPp"
      }
    }
  ]
}